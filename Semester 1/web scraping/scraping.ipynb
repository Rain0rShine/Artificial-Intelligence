{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fde0327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c25a0dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                            Andorra\n",
      "                        \n",
      "\n",
      "\n",
      "                            United Arab Emirates\n",
      "                        \n",
      "\n",
      "\n",
      "                            Afghanistan\n",
      "                        \n",
      "\n",
      "\n",
      "                            Antigua and Barbuda\n",
      "                        \n",
      "\n",
      "\n",
      "                            Anguilla\n",
      "                        \n",
      "\n",
      "\n",
      "                            Albania\n",
      "                        \n",
      "\n",
      "\n",
      "                            Armenia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Angola\n",
      "                        \n",
      "\n",
      "\n",
      "                            Antarctica\n",
      "                        \n",
      "\n",
      "\n",
      "                            Argentina\n",
      "                        \n",
      "\n",
      "\n",
      "                            American Samoa\n",
      "                        \n",
      "\n",
      "\n",
      "                            Austria\n",
      "                        \n",
      "\n",
      "\n",
      "                            Australia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Aruba\n",
      "                        \n",
      "\n",
      "\n",
      "                            Åland\n",
      "                        \n",
      "\n",
      "\n",
      "                            Azerbaijan\n",
      "                        \n",
      "\n",
      "\n",
      "                            Bosnia and Herzegovina\n",
      "                        \n",
      "\n",
      "\n",
      "                            Barbados\n",
      "                        \n",
      "\n",
      "\n",
      "                            Bangladesh\n",
      "                        \n",
      "\n",
      "\n",
      "                            Belgium\n",
      "                        \n",
      "\n",
      "\n",
      "                            Burkina Faso\n",
      "                        \n",
      "\n",
      "\n",
      "                            Bulgaria\n",
      "                        \n",
      "\n",
      "\n",
      "                            Bahrain\n",
      "                        \n",
      "\n",
      "\n",
      "                            Burundi\n",
      "                        \n",
      "\n",
      "\n",
      "                            Benin\n",
      "                        \n",
      "\n",
      "\n",
      "                            Saint Barthélemy\n",
      "                        \n",
      "\n",
      "\n",
      "                            Bermuda\n",
      "                        \n",
      "\n",
      "\n",
      "                            Brunei\n",
      "                        \n",
      "\n",
      "\n",
      "                            Bolivia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Bonaire\n",
      "                        \n",
      "\n",
      "\n",
      "                            Brazil\n",
      "                        \n",
      "\n",
      "\n",
      "                            Bahamas\n",
      "                        \n",
      "\n",
      "\n",
      "                            Bhutan\n",
      "                        \n",
      "\n",
      "\n",
      "                            Bouvet Island\n",
      "                        \n",
      "\n",
      "\n",
      "                            Botswana\n",
      "                        \n",
      "\n",
      "\n",
      "                            Belarus\n",
      "                        \n",
      "\n",
      "\n",
      "                            Belize\n",
      "                        \n",
      "\n",
      "\n",
      "                            Canada\n",
      "                        \n",
      "\n",
      "\n",
      "                            Cocos [Keeling] Islands\n",
      "                        \n",
      "\n",
      "\n",
      "                            Democratic Republic of the Congo\n",
      "                        \n",
      "\n",
      "\n",
      "                            Central African Republic\n",
      "                        \n",
      "\n",
      "\n",
      "                            Republic of the Congo\n",
      "                        \n",
      "\n",
      "\n",
      "                            Switzerland\n",
      "                        \n",
      "\n",
      "\n",
      "                            Ivory Coast\n",
      "                        \n",
      "\n",
      "\n",
      "                            Cook Islands\n",
      "                        \n",
      "\n",
      "\n",
      "                            Chile\n",
      "                        \n",
      "\n",
      "\n",
      "                            Cameroon\n",
      "                        \n",
      "\n",
      "\n",
      "                            China\n",
      "                        \n",
      "\n",
      "\n",
      "                            Colombia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Costa Rica\n",
      "                        \n",
      "\n",
      "\n",
      "                            Cuba\n",
      "                        \n",
      "\n",
      "\n",
      "                            Cape Verde\n",
      "                        \n",
      "\n",
      "\n",
      "                            Curacao\n",
      "                        \n",
      "\n",
      "\n",
      "                            Christmas Island\n",
      "                        \n",
      "\n",
      "\n",
      "                            Cyprus\n",
      "                        \n",
      "\n",
      "\n",
      "                            Czech Republic\n",
      "                        \n",
      "\n",
      "\n",
      "                            Germany\n",
      "                        \n",
      "\n",
      "\n",
      "                            Djibouti\n",
      "                        \n",
      "\n",
      "\n",
      "                            Denmark\n",
      "                        \n",
      "\n",
      "\n",
      "                            Dominica\n",
      "                        \n",
      "\n",
      "\n",
      "                            Dominican Republic\n",
      "                        \n",
      "\n",
      "\n",
      "                            Algeria\n",
      "                        \n",
      "\n",
      "\n",
      "                            Ecuador\n",
      "                        \n",
      "\n",
      "\n",
      "                            Estonia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Egypt\n",
      "                        \n",
      "\n",
      "\n",
      "                            Western Sahara\n",
      "                        \n",
      "\n",
      "\n",
      "                            Eritrea\n",
      "                        \n",
      "\n",
      "\n",
      "                            Spain\n",
      "                        \n",
      "\n",
      "\n",
      "                            Ethiopia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Finland\n",
      "                        \n",
      "\n",
      "\n",
      "                            Fiji\n",
      "                        \n",
      "\n",
      "\n",
      "                            Falkland Islands\n",
      "                        \n",
      "\n",
      "\n",
      "                            Micronesia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Faroe Islands\n",
      "                        \n",
      "\n",
      "\n",
      "                            France\n",
      "                        \n",
      "\n",
      "\n",
      "                            Gabon\n",
      "                        \n",
      "\n",
      "\n",
      "                            United Kingdom\n",
      "                        \n",
      "\n",
      "\n",
      "                            Grenada\n",
      "                        \n",
      "\n",
      "\n",
      "                            Georgia\n",
      "                        \n",
      "\n",
      "\n",
      "                            French Guiana\n",
      "                        \n",
      "\n",
      "\n",
      "                            Guernsey\n",
      "                        \n",
      "\n",
      "\n",
      "                            Ghana\n",
      "                        \n",
      "\n",
      "\n",
      "                            Gibraltar\n",
      "                        \n",
      "\n",
      "\n",
      "                            Greenland\n",
      "                        \n",
      "\n",
      "\n",
      "                            Gambia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Guinea\n",
      "                        \n",
      "\n",
      "\n",
      "                            Guadeloupe\n",
      "                        \n",
      "\n",
      "\n",
      "                            Equatorial Guinea\n",
      "                        \n",
      "\n",
      "\n",
      "                            Greece\n",
      "                        \n",
      "\n",
      "\n",
      "                            South Georgia and the South Sandwich Islands\n",
      "                        \n",
      "\n",
      "\n",
      "                            Guatemala\n",
      "                        \n",
      "\n",
      "\n",
      "                            Guam\n",
      "                        \n",
      "\n",
      "\n",
      "                            Guinea-Bissau\n",
      "                        \n",
      "\n",
      "\n",
      "                            Guyana\n",
      "                        \n",
      "\n",
      "\n",
      "                            Hong Kong\n",
      "                        \n",
      "\n",
      "\n",
      "                            Heard Island and McDonald Islands\n",
      "                        \n",
      "\n",
      "\n",
      "                            Honduras\n",
      "                        \n",
      "\n",
      "\n",
      "                            Croatia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Haiti\n",
      "                        \n",
      "\n",
      "\n",
      "                            Hungary\n",
      "                        \n",
      "\n",
      "\n",
      "                            Indonesia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Ireland\n",
      "                        \n",
      "\n",
      "\n",
      "                            Israel\n",
      "                        \n",
      "\n",
      "\n",
      "                            Isle of Man\n",
      "                        \n",
      "\n",
      "\n",
      "                            India\n",
      "                        \n",
      "\n",
      "\n",
      "                            British Indian Ocean Territory\n",
      "                        \n",
      "\n",
      "\n",
      "                            Iraq\n",
      "                        \n",
      "\n",
      "\n",
      "                            Iran\n",
      "                        \n",
      "\n",
      "\n",
      "                            Iceland\n",
      "                        \n",
      "\n",
      "\n",
      "                            Italy\n",
      "                        \n",
      "\n",
      "\n",
      "                            Jersey\n",
      "                        \n",
      "\n",
      "\n",
      "                            Jamaica\n",
      "                        \n",
      "\n",
      "\n",
      "                            Jordan\n",
      "                        \n",
      "\n",
      "\n",
      "                            Japan\n",
      "                        \n",
      "\n",
      "\n",
      "                            Kenya\n",
      "                        \n",
      "\n",
      "\n",
      "                            Kyrgyzstan\n",
      "                        \n",
      "\n",
      "\n",
      "                            Cambodia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Kiribati\n",
      "                        \n",
      "\n",
      "\n",
      "                            Comoros\n",
      "                        \n",
      "\n",
      "\n",
      "                            Saint Kitts and Nevis\n",
      "                        \n",
      "\n",
      "\n",
      "                            North Korea\n",
      "                        \n",
      "\n",
      "\n",
      "                            South Korea\n",
      "                        \n",
      "\n",
      "\n",
      "                            Kuwait\n",
      "                        \n",
      "\n",
      "\n",
      "                            Cayman Islands\n",
      "                        \n",
      "\n",
      "\n",
      "                            Kazakhstan\n",
      "                        \n",
      "\n",
      "\n",
      "                            Laos\n",
      "                        \n",
      "\n",
      "\n",
      "                            Lebanon\n",
      "                        \n",
      "\n",
      "\n",
      "                            Saint Lucia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Liechtenstein\n",
      "                        \n",
      "\n",
      "\n",
      "                            Sri Lanka\n",
      "                        \n",
      "\n",
      "\n",
      "                            Liberia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Lesotho\n",
      "                        \n",
      "\n",
      "\n",
      "                            Lithuania\n",
      "                        \n",
      "\n",
      "\n",
      "                            Luxembourg\n",
      "                        \n",
      "\n",
      "\n",
      "                            Latvia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Libya\n",
      "                        \n",
      "\n",
      "\n",
      "                            Morocco\n",
      "                        \n",
      "\n",
      "\n",
      "                            Monaco\n",
      "                        \n",
      "\n",
      "\n",
      "                            Moldova\n",
      "                        \n",
      "\n",
      "\n",
      "                            Montenegro\n",
      "                        \n",
      "\n",
      "\n",
      "                            Saint Martin\n",
      "                        \n",
      "\n",
      "\n",
      "                            Madagascar\n",
      "                        \n",
      "\n",
      "\n",
      "                            Marshall Islands\n",
      "                        \n",
      "\n",
      "\n",
      "                            Macedonia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Mali\n",
      "                        \n",
      "\n",
      "\n",
      "                            Myanmar [Burma]\n",
      "                        \n",
      "\n",
      "\n",
      "                            Mongolia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Macao\n",
      "                        \n",
      "\n",
      "\n",
      "                            Northern Mariana Islands\n",
      "                        \n",
      "\n",
      "\n",
      "                            Martinique\n",
      "                        \n",
      "\n",
      "\n",
      "                            Mauritania\n",
      "                        \n",
      "\n",
      "\n",
      "                            Montserrat\n",
      "                        \n",
      "\n",
      "\n",
      "                            Malta\n",
      "                        \n",
      "\n",
      "\n",
      "                            Mauritius\n",
      "                        \n",
      "\n",
      "\n",
      "                            Maldives\n",
      "                        \n",
      "\n",
      "\n",
      "                            Malawi\n",
      "                        \n",
      "\n",
      "\n",
      "                            Mexico\n",
      "                        \n",
      "\n",
      "\n",
      "                            Malaysia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Mozambique\n",
      "                        \n",
      "\n",
      "\n",
      "                            Namibia\n",
      "                        \n",
      "\n",
      "\n",
      "                            New Caledonia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Niger\n",
      "                        \n",
      "\n",
      "\n",
      "                            Norfolk Island\n",
      "                        \n",
      "\n",
      "\n",
      "                            Nigeria\n",
      "                        \n",
      "\n",
      "\n",
      "                            Nicaragua\n",
      "                        \n",
      "\n",
      "\n",
      "                            Netherlands\n",
      "                        \n",
      "\n",
      "\n",
      "                            Norway\n",
      "                        \n",
      "\n",
      "\n",
      "                            Nepal\n",
      "                        \n",
      "\n",
      "\n",
      "                            Nauru\n",
      "                        \n",
      "\n",
      "\n",
      "                            Niue\n",
      "                        \n",
      "\n",
      "\n",
      "                            New Zealand\n",
      "                        \n",
      "\n",
      "\n",
      "                            Oman\n",
      "                        \n",
      "\n",
      "\n",
      "                            Panama\n",
      "                        \n",
      "\n",
      "\n",
      "                            Peru\n",
      "                        \n",
      "\n",
      "\n",
      "                            French Polynesia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Papua New Guinea\n",
      "                        \n",
      "\n",
      "\n",
      "                            Philippines\n",
      "                        \n",
      "\n",
      "\n",
      "                            Pakistan\n",
      "                        \n",
      "\n",
      "\n",
      "                            Poland\n",
      "                        \n",
      "\n",
      "\n",
      "                            Saint Pierre and Miquelon\n",
      "                        \n",
      "\n",
      "\n",
      "                            Pitcairn Islands\n",
      "                        \n",
      "\n",
      "\n",
      "                            Puerto Rico\n",
      "                        \n",
      "\n",
      "\n",
      "                            Palestine\n",
      "                        \n",
      "\n",
      "\n",
      "                            Portugal\n",
      "                        \n",
      "\n",
      "\n",
      "                            Palau\n",
      "                        \n",
      "\n",
      "\n",
      "                            Paraguay\n",
      "                        \n",
      "\n",
      "\n",
      "                            Qatar\n",
      "                        \n",
      "\n",
      "\n",
      "                            Réunion\n",
      "                        \n",
      "\n",
      "\n",
      "                            Romania\n",
      "                        \n",
      "\n",
      "\n",
      "                            Serbia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Russia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Rwanda\n",
      "                        \n",
      "\n",
      "\n",
      "                            Saudi Arabia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Solomon Islands\n",
      "                        \n",
      "\n",
      "\n",
      "                            Seychelles\n",
      "                        \n",
      "\n",
      "\n",
      "                            Sudan\n",
      "                        \n",
      "\n",
      "\n",
      "                            Sweden\n",
      "                        \n",
      "\n",
      "\n",
      "                            Singapore\n",
      "                        \n",
      "\n",
      "\n",
      "                            Saint Helena\n",
      "                        \n",
      "\n",
      "\n",
      "                            Slovenia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Svalbard and Jan Mayen\n",
      "                        \n",
      "\n",
      "\n",
      "                            Slovakia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Sierra Leone\n",
      "                        \n",
      "\n",
      "\n",
      "                            San Marino\n",
      "                        \n",
      "\n",
      "\n",
      "                            Senegal\n",
      "                        \n",
      "\n",
      "\n",
      "                            Somalia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Suriname\n",
      "                        \n",
      "\n",
      "\n",
      "                            South Sudan\n",
      "                        \n",
      "\n",
      "\n",
      "                            São Tomé and Príncipe\n",
      "                        \n",
      "\n",
      "\n",
      "                            El Salvador\n",
      "                        \n",
      "\n",
      "\n",
      "                            Sint Maarten\n",
      "                        \n",
      "\n",
      "\n",
      "                            Syria\n",
      "                        \n",
      "\n",
      "\n",
      "                            Swaziland\n",
      "                        \n",
      "\n",
      "\n",
      "                            Turks and Caicos Islands\n",
      "                        \n",
      "\n",
      "\n",
      "                            Chad\n",
      "                        \n",
      "\n",
      "\n",
      "                            French Southern Territories\n",
      "                        \n",
      "\n",
      "\n",
      "                            Togo\n",
      "                        \n",
      "\n",
      "\n",
      "                            Thailand\n",
      "                        \n",
      "\n",
      "\n",
      "                            Tajikistan\n",
      "                        \n",
      "\n",
      "\n",
      "                            Tokelau\n",
      "                        \n",
      "\n",
      "\n",
      "                            East Timor\n",
      "                        \n",
      "\n",
      "\n",
      "                            Turkmenistan\n",
      "                        \n",
      "\n",
      "\n",
      "                            Tunisia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Tonga\n",
      "                        \n",
      "\n",
      "\n",
      "                            Turkey\n",
      "                        \n",
      "\n",
      "\n",
      "                            Trinidad and Tobago\n",
      "                        \n",
      "\n",
      "\n",
      "                            Tuvalu\n",
      "                        \n",
      "\n",
      "\n",
      "                            Taiwan\n",
      "                        \n",
      "\n",
      "\n",
      "                            Tanzania\n",
      "                        \n",
      "\n",
      "\n",
      "                            Ukraine\n",
      "                        \n",
      "\n",
      "\n",
      "                            Uganda\n",
      "                        \n",
      "\n",
      "\n",
      "                            U.S. Minor Outlying Islands\n",
      "                        \n",
      "\n",
      "\n",
      "                            United States\n",
      "                        \n",
      "\n",
      "\n",
      "                            Uruguay\n",
      "                        \n",
      "\n",
      "\n",
      "                            Uzbekistan\n",
      "                        \n",
      "\n",
      "\n",
      "                            Vatican City\n",
      "                        \n",
      "\n",
      "\n",
      "                            Saint Vincent and the Grenadines\n",
      "                        \n",
      "\n",
      "\n",
      "                            Venezuela\n",
      "                        \n",
      "\n",
      "\n",
      "                            British Virgin Islands\n",
      "                        \n",
      "\n",
      "\n",
      "                            U.S. Virgin Islands\n",
      "                        \n",
      "\n",
      "\n",
      "                            Vietnam\n",
      "                        \n",
      "\n",
      "\n",
      "                            Vanuatu\n",
      "                        \n",
      "\n",
      "\n",
      "                            Wallis and Futuna\n",
      "                        \n",
      "\n",
      "\n",
      "                            Samoa\n",
      "                        \n",
      "\n",
      "\n",
      "                            Kosovo\n",
      "                        \n",
      "\n",
      "\n",
      "                            Yemen\n",
      "                        \n",
      "\n",
      "\n",
      "                            Mayotte\n",
      "                        \n",
      "\n",
      "\n",
      "                            South Africa\n",
      "                        \n",
      "\n",
      "\n",
      "                            Zambia\n",
      "                        \n",
      "\n",
      "\n",
      "                            Zimbabwe\n",
      "                        \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Send a request to the website and get the HTML content\n",
    "url = 'https://www.scrapethissite.com/pages/simple/'  # Replace with the URL of the website you're scraping\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Step 3: Find all <h3> tags\n",
    "h3_tags = soup.find_all('h3')\n",
    "\n",
    "# Step 4: Print the content of each <h3> tag\n",
    "for h3 in h3_tags:\n",
    "    print(h3.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f7c481d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    }
   ],
   "source": [
    "print(len(h3_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a72c264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = []\n",
    "for h3 in h3_tags:\n",
    "    country = h3.get_text()\n",
    "    country = country.strip()\n",
    "    countries.append(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28945d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "containsA = 0\n",
    "for i in range(len(countries)):\n",
    "    if 'a' in country:\n",
    "        containsA += 1\n",
    "containsA        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "594f900f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463\n"
     ]
    }
   ],
   "source": [
    "for country in countries:\n",
    "    if 'a' in country.lower():  # Using .lower() to make the search case-insensitive\n",
    "        containsA += 1\n",
    "\n",
    "print(containsA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1b34f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total links: 2786\n",
      "Internal links (Wikipedia articles): 1468 (52.69%)\n",
      "External links: 595 (21.36%)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Step 1: Get the page content\n",
    "url = \"https://en.wikipedia.org/wiki/Andorra\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Step 2: Extract all links\n",
    "links = soup.find_all('a', href=True)\n",
    "\n",
    "# Step 3: Categorize the links\n",
    "internal_links = 0\n",
    "external_links = 0\n",
    "total_links = len(links)\n",
    "\n",
    "for link in links:\n",
    "    href = link['href']\n",
    "    if href.startswith('/wiki/'):  # It's an internal Wikipedia link\n",
    "        internal_links += 1\n",
    "    elif href.startswith('http') or href.startswith('www'):  # It's an external link\n",
    "        external_links += 1\n",
    "\n",
    "# Step 4: Calculate the proportion\n",
    "internal_proportion = internal_links / total_links if total_links else 0\n",
    "external_proportion = external_links / total_links if total_links else 0\n",
    "\n",
    "# Step 5: Print the results\n",
    "print(f\"Total links: {total_links}\")\n",
    "print(f\"Internal links (Wikipedia articles): {internal_links} ({internal_proportion:.2%})\")\n",
    "print(f\"External links: {external_links} ({external_proportion:.2%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee472262",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.scrapethissite.com/pages/forms/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e7a4d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find('table', class_='table table-bordered table-striped table-condensed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee0113f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8656\\2503182262.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Step 5: Initialize lists to hold team names and wins\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mteams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "rows = table.find_all('tr')\n",
    "\n",
    "# Step 5: Initialize lists to hold team names and wins\n",
    "teams = []\n",
    "wins = []\n",
    "\n",
    "# Step 6: Loop through each row and extract team and wins information\n",
    "for row in rows[1:]:  # Skip the first row (header)\n",
    "    cols = row.find_all('td')\n",
    "    if len(cols) > 1:\n",
    "        team_name = cols[0].text.strip()  # The first column is the team name\n",
    "        win_count = int(cols[1].text.strip())  # The second column is the win count\n",
    "        teams.append(team_name)\n",
    "        wins.append(win_count)\n",
    "\n",
    "# Step 7: Create a DataFrame from the lists\n",
    "data = {'Team': teams, 'Wins': wins}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 8: Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5e7b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Send a request to the website and get the HTML content\n",
    "url = 'https://www.scrapethissite.com/pages/forms/'  # Replace with the URL of the website you're scraping\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Step 3: Find all <h3> tags\n",
    "h3_tags = soup.find_all('h3')\n",
    "\n",
    "# Step 4: Print the content of each <h3> tag\n",
    "for h3 in h3_tags:\n",
    "    print(h3.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87505df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f12331",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' https://en.wikipedia.org/wiki/List_of_Nobel_laureates_by_country#Nobel_Prizes_by_category/country_of_birth'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61d018e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Send a request to the website and get the HTML content\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_Nobel_laureates'  # Replace with the URL of the website you're scraping\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "mydivs = soup.find_all(\"div\", {\"class\": \"vcard\"})\n",
    "for vcard in mydivs:\n",
    "    print(vcard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef4417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "124c23c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobus Henricus van 't Hoff\n",
      "Emil Fischer\n",
      "Svante Arrhenius\n",
      "William Ramsay\n",
      "Adolf von Baeyer\n",
      "Henri Moissan\n",
      "Eduard Buchner\n",
      "Ernest Rutherford\n",
      "Wilhelm Ostwald\n",
      "Otto Wallach\n",
      "Marie Curie\n",
      "Victor Grignard\n",
      "Alfred Werner\n",
      "Theodore William Richards\n",
      "Richard Willstätter\n",
      "Fritz Haber\n",
      "Walther Nernst\n",
      "Frederick Soddy\n",
      "Francis William Aston\n",
      "Fritz Pregl\n",
      "Richard Adolf Zsigmondy\n",
      "Theodor Svedberg\n",
      "Heinrich Otto Wieland\n",
      "Adolf Windaus\n",
      "Arthur Harden\n",
      "Hans Fischer\n",
      "Carl Bosch\n",
      "Irving Langmuir\n",
      "Harold Urey\n",
      "Frédéric Joliot-Curie\n",
      "Peter Debye\n",
      "Norman Haworth\n",
      "Richard Kuhn\n",
      "Adolf Butenandt\n",
      "George de Hevesy\n",
      "Otto Hahn\n",
      "Artturi Ilmari Virtanen\n",
      "James B. Sumner\n",
      "Robert Robinson\n",
      "Arne Tiselius\n",
      "William Giauque\n",
      "Otto Diels\n",
      "Edwin McMillan\n",
      "Archer Martin\n",
      "Hermann Staudinger\n",
      "Linus Pauling\n",
      "Vincent du Vigneaud\n",
      "Cyril Norman Hinshelwood\n",
      "The Lord Todd\n",
      "Frederick Sanger\n",
      "Jaroslav Heyrovský\n",
      "Willard Libby\n",
      "Melvin Calvin\n",
      "Max Perutz\n",
      "Karl Ziegler\n",
      "Dorothy Hodgkin\n",
      "Robert Burns Woodward\n",
      "Robert S. Mulliken\n",
      "Manfred Eigen\n",
      "Lars Onsager\n",
      "Derek Barton\n",
      "Luis Federico Leloir\n",
      "Gerhard Herzberg\n",
      "Christian B. Anfinsen\n",
      "Ernst Otto Fischer\n",
      "Paul Flory\n",
      "John Cornforth\n",
      "William Lipscomb\n",
      "Ilya Prigogine\n",
      "Peter D. Mitchell\n",
      "Herbert C. Brown\n",
      "Paul Berg\n",
      "Kenichi Fukui\n",
      "Aaron Klug\n",
      "Henry Taube\n",
      "Robert Bruce Merrifield\n",
      "Herbert A. Hauptman\n",
      "Dudley R. Herschbach\n",
      "Donald J. Cram\n",
      "Johann Deisenhofer\n",
      "Sidney Altman\n",
      "Elias James Corey\n",
      "Richard R. Ernst\n",
      "Rudolph A. Marcus\n",
      "Kary Mullis\n",
      "George Andrew Olah\n",
      "Paul J. Crutzen\n",
      "Robert Curl\n",
      "Paul D. Boyer\n",
      "Walter Kohn\n",
      "Ahmed Zewail\n",
      "Alan J. Heeger\n",
      "William Standish Knowles\n",
      "John B. Fenn\n",
      "Peter Agre\n",
      "Aaron Ciechanover\n",
      "Yves Chauvin\n",
      "Roger D. Kornberg\n",
      "Gerhard Ertl\n",
      "Osamu Shimomura\n",
      "Venkatraman Ramakrishnan\n",
      "Richard F. Heck\n",
      "Dan Shechtman\n",
      "Brian K. Kobilka\n",
      "Martin Karplus\n",
      "Eric Betzig\n",
      "Tomas Lindahl\n",
      "Jean-Pierre Sauvage\n",
      "Jacques Dubochet\n",
      "Frances H. Arnold\n",
      "John B. Goodenough\n",
      "Emmanuelle Charpentier\n",
      "Benjamin List\n",
      "Carolyn Bertozzi\n",
      "Moungi Bawendi\n",
      "David Baker\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Send a request to the website and get the HTML content\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_Nobel_laureates'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Step 3: Find the tables that list Nobel laureates\n",
    "# Look for tables with a specific class that holds the laureates' names (the class may vary)\n",
    "tables = soup.find_all('table', {'class': 'wikitable'})\n",
    "\n",
    "# Step 4: Loop through each table and extract names\n",
    "for table in tables:\n",
    "    # Find all rows in the table\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    # Loop through each row and extract the name (usually in an <a> tag in the second column)\n",
    "    for row in rows:\n",
    "        # Find all table data cells (td)\n",
    "        cols = row.find_all('td')\n",
    "        \n",
    "        # Ensure there are enough columns in the row\n",
    "        if len(cols) > 1:\n",
    "            # Try to find a link (<a>) in the second column (which usually contains the name)\n",
    "            name_tag = cols[1].find('a')\n",
    "            if name_tag:\n",
    "                # Extract the name from the <a> tag\n",
    "                print(name_tag.get_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f77082d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(URL):\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    str_soup = str(soup)\n",
    "    if \"Princeton\" in str_soup:\n",
    "        return True\n",
    "        print(\"true\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16dfc364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy University found on https://en.wikipedia.org/wiki/Jacobus_Henricus_van_%27t_Hoff\n",
      "Princeton found for Jacobus Henricus van 't Hoff: https://en.wikipedia.org/wiki/Jacobus_Henricus_van_%27t_Hoff\n",
      "Princeton not found for Emil Fischer.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Svante_Arrhenius\n",
      "Princeton found for Svante Arrhenius: https://en.wikipedia.org/wiki/Svante_Arrhenius\n",
      "Princeton not found for William Ramsay.\n",
      "Princeton not found for Adolf von Baeyer.\n",
      "Princeton not found for Henri Moissan.\n",
      "Princeton not found for Eduard Buchner.\n",
      "Princeton not found for Ernest Rutherford.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Wilhelm_Ostwald\n",
      "Princeton found for Wilhelm Ostwald: https://en.wikipedia.org/wiki/Wilhelm_Ostwald\n",
      "Princeton not found for Otto Wallach.\n",
      "Princeton not found for Marie Curie.\n",
      "Princeton not found for Victor Grignard.\n",
      "Princeton not found for Alfred Werner.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Theodore_William_Richards\n",
      "Princeton found for Theodore William Richards: https://en.wikipedia.org/wiki/Theodore_William_Richards\n",
      "Princeton not found for Richard Willstätter.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Fritz_Haber\n",
      "Princeton found for Fritz Haber: https://en.wikipedia.org/wiki/Fritz_Haber\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Walther_Nernst\n",
      "Princeton found for Walther Nernst: https://en.wikipedia.org/wiki/Walther_Nernst\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Frederick_Soddy\n",
      "Princeton found for Frederick Soddy: https://en.wikipedia.org/wiki/Frederick_Soddy\n",
      "Princeton not found for Francis William Aston.\n",
      "Princeton not found for Fritz Pregl.\n",
      "Princeton not found for Richard Adolf Zsigmondy.\n",
      "Princeton not found for Theodor Svedberg.\n",
      "Princeton not found for Heinrich Otto Wieland.\n",
      "Princeton not found for Adolf Windaus.\n",
      "Princeton not found for Arthur Harden.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Hans_Fischer\n",
      "Princeton found for Hans Fischer: https://en.wikipedia.org/wiki/Hans_Fischer\n",
      "Princeton not found for Carl Bosch.\n",
      "Princeton not found for Irving Langmuir.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Harold_Urey\n",
      "Princeton found for Harold Urey: https://en.wikipedia.org/wiki/Harold_Urey\n",
      "Princeton not found for Frédéric Joliot-Curie.\n",
      "Princeton not found for Peter Debye.\n",
      "Princeton not found for Norman Haworth.\n",
      "Princeton not found for Richard Kuhn.\n",
      "Princeton not found for Adolf Butenandt.\n",
      "Princeton not found for George de Hevesy.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Otto_Hahn\n",
      "Princeton found for Otto Hahn: https://en.wikipedia.org/wiki/Otto_Hahn\n",
      "Princeton not found for Artturi Ilmari Virtanen.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/James_B._Sumner\n",
      "Princeton found for James B. Sumner: https://en.wikipedia.org/wiki/James_B._Sumner\n",
      "Princeton not found for Robert Robinson.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Arne_Tiselius\n",
      "Princeton found for Arne Tiselius: https://en.wikipedia.org/wiki/Arne_Tiselius\n",
      "Princeton not found for William Giauque.\n",
      "Princeton not found for Otto Diels.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Edwin_McMillan\n",
      "Princeton found for Edwin McMillan: https://en.wikipedia.org/wiki/Edwin_McMillan\n",
      "Princeton not found for Archer Martin.\n",
      "Princeton not found for Hermann Staudinger.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Linus_Pauling\n",
      "Princeton found for Linus Pauling: https://en.wikipedia.org/wiki/Linus_Pauling\n",
      "Princeton not found for Vincent du Vigneaud.\n",
      "Princeton not found for Cyril Norman Hinshelwood.\n",
      "Princeton not found for The Lord Todd.\n",
      "Princeton not found for Frederick Sanger.\n",
      "Princeton not found for Jaroslav Heyrovský.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Willard_Libby\n",
      "Princeton found for Willard Libby: https://en.wikipedia.org/wiki/Willard_Libby\n",
      "Princeton not found for Melvin Calvin.\n",
      "Princeton not found for Max Perutz.\n",
      "Princeton not found for Karl Ziegler.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Dorothy_Hodgkin\n",
      "Princeton found for Dorothy Hodgkin: https://en.wikipedia.org/wiki/Dorothy_Hodgkin\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Robert_Burns_Woodward\n",
      "Princeton found for Robert Burns Woodward: https://en.wikipedia.org/wiki/Robert_Burns_Woodward\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Robert_S._Mulliken\n",
      "Princeton found for Robert S. Mulliken: https://en.wikipedia.org/wiki/Robert_S._Mulliken\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Manfred_Eigen\n",
      "Princeton found for Manfred Eigen: https://en.wikipedia.org/wiki/Manfred_Eigen\n",
      "Princeton not found for Lars Onsager.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Derek_Barton\n",
      "Princeton found for Derek Barton: https://en.wikipedia.org/wiki/Derek_Barton\n",
      "Princeton not found for Luis Federico Leloir.\n",
      "Princeton not found for Gerhard Herzberg.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Christian_B._Anfinsen\n",
      "Princeton found for Christian B. Anfinsen: https://en.wikipedia.org/wiki/Christian_B._Anfinsen\n",
      "Princeton not found for Ernst Otto Fischer.\n",
      "Princeton not found for Paul Flory.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/John_Cornforth\n",
      "Princeton found for John Cornforth: https://en.wikipedia.org/wiki/John_Cornforth\n",
      "Ivy University found on https://en.wikipedia.org/wiki/William_Lipscomb\n",
      "Princeton found for William Lipscomb: https://en.wikipedia.org/wiki/William_Lipscomb\n",
      "Princeton not found for Ilya Prigogine.\n",
      "Princeton not found for Peter D. Mitchell.\n",
      "Princeton not found for Herbert C. Brown.\n",
      "Princeton not found for Paul Berg.\n",
      "Princeton not found for Kenichi Fukui.\n",
      "Princeton not found for Aaron Klug.\n",
      "Princeton not found for Henry Taube.\n",
      "Princeton not found for Robert Bruce Merrifield.\n",
      "Princeton not found for Herbert A. Hauptman.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Dudley_R._Herschbach\n",
      "Princeton found for Dudley R. Herschbach: https://en.wikipedia.org/wiki/Dudley_R._Herschbach\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Donald_J._Cram\n",
      "Princeton found for Donald J. Cram: https://en.wikipedia.org/wiki/Donald_J._Cram\n",
      "Princeton not found for Johann Deisenhofer.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Sidney_Altman\n",
      "Princeton found for Sidney Altman: https://en.wikipedia.org/wiki/Sidney_Altman\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Elias_James_Corey\n",
      "Princeton found for Elias James Corey: https://en.wikipedia.org/wiki/Elias_James_Corey\n",
      "Princeton not found for Richard R. Ernst.\n",
      "Princeton not found for Rudolph A. Marcus.\n",
      "Princeton not found for Kary Mullis.\n",
      "Princeton not found for George Andrew Olah.\n",
      "Princeton not found for Paul J. Crutzen.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Robert_Curl\n",
      "Princeton found for Robert Curl: https://en.wikipedia.org/wiki/Robert_Curl\n",
      "Princeton not found for Paul D. Boyer.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Walter_Kohn\n",
      "Princeton found for Walter Kohn: https://en.wikipedia.org/wiki/Walter_Kohn\n",
      "Princeton not found for Ahmed Zewail.\n",
      "Princeton not found for Alan J. Heeger.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/William_Standish_Knowles\n",
      "Princeton found for William Standish Knowles: https://en.wikipedia.org/wiki/William_Standish_Knowles\n",
      "Ivy University found on https://en.wikipedia.org/wiki/John_B._Fenn\n",
      "Princeton found for John B. Fenn: https://en.wikipedia.org/wiki/John_B._Fenn\n",
      "Princeton not found for Peter Agre.\n",
      "Princeton not found for Aaron Ciechanover.\n",
      "Princeton not found for Yves Chauvin.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Roger_D._Kornberg\n",
      "Princeton found for Roger D. Kornberg: https://en.wikipedia.org/wiki/Roger_D._Kornberg\n",
      "Princeton not found for Gerhard Ertl.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Osamu_Shimomura\n",
      "Princeton found for Osamu Shimomura: https://en.wikipedia.org/wiki/Osamu_Shimomura\n",
      "Princeton not found for Venkatraman Ramakrishnan.\n",
      "Princeton not found for Richard F. Heck.\n",
      "Princeton not found for Dan Shechtman.\n",
      "Princeton not found for Brian K. Kobilka.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Martin_Karplus\n",
      "Princeton found for Martin Karplus: https://en.wikipedia.org/wiki/Martin_Karplus\n",
      "Princeton not found for Eric Betzig.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy University found on https://en.wikipedia.org/wiki/Tomas_Lindahl\n",
      "Princeton found for Tomas Lindahl: https://en.wikipedia.org/wiki/Tomas_Lindahl\n",
      "Princeton not found for Jean-Pierre Sauvage.\n",
      "Princeton not found for Jacques Dubochet.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Frances_H._Arnold\n",
      "Princeton found for Frances H. Arnold: https://en.wikipedia.org/wiki/Frances_H._Arnold\n",
      "Ivy University found on https://en.wikipedia.org/wiki/John_B._Goodenough\n",
      "Princeton found for John B. Goodenough: https://en.wikipedia.org/wiki/John_B._Goodenough\n",
      "Princeton not found for Emmanuelle Charpentier.\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Benjamin_List\n",
      "Princeton found for Benjamin List: https://en.wikipedia.org/wiki/Benjamin_List\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Carolyn_Bertozzi\n",
      "Princeton found for Carolyn Bertozzi: https://en.wikipedia.org/wiki/Carolyn_Bertozzi\n",
      "Ivy University found on https://en.wikipedia.org/wiki/Moungi_Bawendi\n",
      "Princeton found for Moungi Bawendi: https://en.wikipedia.org/wiki/Moungi_Bawendi\n",
      "Ivy University found on https://en.wikipedia.org/wiki/David_Baker_(biochemist)\n",
      "Princeton found for David Baker: https://en.wikipedia.org/wiki/David_Baker_(biochemist)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to check if the page contains \"Princeton\"\n",
    "def scrape_page(URL):\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    str_soup = str(soup)\n",
    "    \n",
    "    if \"Princeton\" in str_soup or \"Harvard\" in str_soup:\n",
    "        print(f\"Ivy League University found on {URL}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Step 1: Send a request to the website and get the HTML content\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_Nobel_laureates'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Step 2: Find the tables that list Nobel laureates\n",
    "tables = soup.find_all('table', {'class': 'wikitable'})\n",
    "\n",
    "# Step 3: Loop through each table and extract laureates' names and links\n",
    "for table in tables:\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        \n",
    "        if len(cols) > 1:\n",
    "            # Find the link (<a>) in the second column (usually the name of the laureate)\n",
    "            name_tag = cols[1].find('a')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text()\n",
    "                link = 'https://en.wikipedia.org' + name_tag.get('href')\n",
    "\n",
    "                # Step 4: Visit each laureate's page and check for \"Princeton\"\n",
    "                if scrape_page(link):\n",
    "                    print(f\"Princeton found for {name}: {link}\")\n",
    "                else:\n",
    "                    print(f\"Princeton not found for {name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b17a1e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Send a request to the website and get the HTML content\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_Nobel_laureates'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Step 2: Find the tables that list Nobel laureates\n",
    "tables = soup.find_all('table', {'class': 'wikitable'})\n",
    "\n",
    "# Step 3: Loop through each table and extract the laureates' names and links\n",
    "for table in tables:\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        \n",
    "        if len(cols) > 1:\n",
    "            # Find the link (<a>) in the second column\n",
    "            name_tag = cols[1].find('a')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text()\n",
    "                link = 'https://en.wikipedia.org' + name_tag.get('href')\n",
    "                \n",
    "\n",
    "                # Step 4: Visit each laureate's page for more details\n",
    "                laureate_response = requests.get(link)\n",
    "                laureate_soup = BeautifulSoup(laureate_response.text, 'html.parser')\n",
    "                scrape_page(link)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb236c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Send a request to the website and get the HTML content\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_Nobel_laureates'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Step 2: Find the tables that list Nobel laureates\n",
    "tables = soup.find_all('table', {'class': 'wikitable'})\n",
    "\n",
    "# Step 3: Loop through each table and extract the laureates' names and links\n",
    "for table in tables:\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        \n",
    "        if len(cols) > 1:\n",
    "            # Find the link (<a>) in the second column\n",
    "            name_tag = cols[1].find('a')\n",
    "            if name_tag:\n",
    "                name = name_tag.get_text()\n",
    "                link = 'https://en.wikipedia.org' + name_tag.get('href')\n",
    "                print(f\"Name: {name}, Link: {link}\")\n",
    "\n",
    "                # Step 4: Visit each laureate's page for more details\n",
    "                laureate_response = requests.get(link)\n",
    "                laureate_soup = BeautifulSoup(laureate_response.text, 'html.parser')\n",
    "\n",
    "                # Example: Extract the first paragraph (bio or intro)\n",
    "                first_paragraph = laureate_soup.find('p')\n",
    "                if first_paragraph:\n",
    "                    print(\"Bio:\", first_paragraph.get_text())\n",
    "                print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b9228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Step 1: Send a request to the Wikipedia page and get the HTML content\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_Nobel_laureates'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Step 3: Find the tables that list Nobel laureates\n",
    "tables = soup.find_all('table', {'class': 'wikitable'})\n",
    "\n",
    "# Step 4: Loop through each table and extract names and their corresponding links\n",
    "for table in tables:\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        \n",
    "        if len(cols) > 1:\n",
    "            name_tag = cols[1].find('a')  # Laureate's name is typically in the second column (index 1)\n",
    "            if name_tag:\n",
    "                # Get the text of the name and the href (URL) to the laureate's page\n",
    "                name = name_tag.get_text()\n",
    "                relative_url = name_tag.get('href')\n",
    "                \n",
    "                # Construct the full URL for the laureate's page\n",
    "                full_url = f'https://en.wikipedia.org{relative_url}'\n",
    "                \n",
    "                print(f\"Visiting: {name} - {full_url}\")\n",
    "                \n",
    "                # Step 5: Send a request to the individual laureate's page and get the content\n",
    "                laureate_response = requests.get(full_url)\n",
    "                laureate_soup = BeautifulSoup(laureate_response.text, 'html.parser')\n",
    "\n",
    "                # Step 6: Find the \"Institutions\" section or similar\n",
    "                institutions_section = None\n",
    "\n",
    "                # Look for headers with titles like \"Institutions\" or similar\n",
    "                headers = laureate_soup.find_all(['span', 'h2'], text=[\"Institutions\", \"Academic career\", \"Affiliations\"])\n",
    "\n",
    "                if headers:\n",
    "                    for header in headers:\n",
    "                        if header.name == 'span' and header.get('id') == 'Institutions':\n",
    "                            # Get the following sibling elements that contain the text\n",
    "                            institutions_section = header.find_next('p')  # The next <p> tag is likely to contain institutions info\n",
    "                            break\n",
    "\n",
    "                if institutions_section:\n",
    "                    # Print the institutions-related paragraph\n",
    "                    print(f\"Institutions Information for {name}: {institutions_section.get_text()}\\n\")\n",
    "                else:\n",
    "                    print(f\"No institutions information found for {name}.\\n\")\n",
    "                    '' break '''\n",
    "\n",
    "                # Sleep to avoid sending too many requests in a short time\n",
    "                time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527423b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Step 1: Send a request to the Wikipedia page and get the HTML content\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_Nobel_laureates'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Step 3: Find the tables that list Nobel laureates\n",
    "tables = soup.find_all('table', {'class': 'wikitable'})\n",
    "\n",
    "# Step 4: Loop through each table and extract names and their corresponding links\n",
    "for table in tables:\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        \n",
    "        if len(cols) > 1:\n",
    "            name_tag = cols[1].find('a')  # Laureate's name is typically in the second column (index 1)\n",
    "            if name_tag:\n",
    "                # Get the text of the name and the href (URL) to the laureate's page\n",
    "                name = name_tag.get_text()\n",
    "                relative_url = name_tag.get('href')\n",
    "                \n",
    "                # Construct the full URL for the laureate's page\n",
    "                full_url = f'https://en.wikipedia.org{relative_url}'\n",
    "                \n",
    "                print(f\"Visiting: {name} - {full_url}\")\n",
    "                \n",
    "                # Step 5: Send a request to the individual laureate's page and get the content\n",
    "                laureate_response = requests.get(full_url)\n",
    "                laureate_soup = BeautifulSoup(laureate_response.text, 'html.parser')\n",
    "\n",
    "                # Step 6: Find the \"Institutions\" section or similar\n",
    "                institutions_section = None\n",
    "\n",
    "                # Look for the header containing \"Institutions\" or similar words\n",
    "                headers = laureate_soup.find_all(['span', 'h2'], string=lambda text: text and 'Institutions' in text)\n",
    "\n",
    "                if headers:\n",
    "                    for header in headers:\n",
    "                        # Look for the first <p> after the header containing institutions info\n",
    "                        institutions_section = header.find_next('p')  # Get the next <p> tag after the header\n",
    "                        break  # Only process the first \"Institutions\" section we find\n",
    "\n",
    "                if institutions_section:\n",
    "                    # Print the institutions-related paragraph\n",
    "                    print(f\"Institutions Information for {name}: {institutions_section.get_text()}\\n\")\n",
    "                else:\n",
    "                    print(f\"No institutions information found for {name}.\\n\")\n",
    "       \n",
    "\n",
    "                # Sleep to avoid sending too many requests in a short time\n",
    "                time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34356245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Step 1: Send a request to the Wikipedia page and get the HTML content\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_Nobel_laureates'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Step 3: Find the tables that list Nobel laureates\n",
    "tables = soup.find_all('table', {'class': 'wikitable'})\n",
    "\n",
    "# Step 4: Loop through each table and extract names and their corresponding links\n",
    "for table in tables:\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        \n",
    "        if len(cols) > 1:\n",
    "            name_tag = cols[1].find('a')  # Laureate's name is typically in the second column (index 1)\n",
    "            if name_tag:\n",
    "                # Get the text of the name and the href (URL) to the laureate's page\n",
    "                name = name_tag.get_text()\n",
    "                relative_url = name_tag.get('href')\n",
    "                \n",
    "                # Construct the full URL for the laureate's page\n",
    "                full_url = f'https://en.wikipedia.org{relative_url}'\n",
    "                \n",
    "                print(f\"Visiting: {name} - {full_url}\")\n",
    "                \n",
    "                # Step 5: Send a request to the individual laureate's page and get the content\n",
    "                laureate_response = requests.get(full_url)\n",
    "                laureate_soup = BeautifulSoup(laureate_response.text, 'html.parser')\n",
    "\n",
    "                # Step 6: Find the \"Institutions\" section or similar\n",
    "                institutions_section = None\n",
    "\n",
    "                # Debug: Print the first 500 characters of the HTML to check the structure\n",
    "                # print(laureate_soup.prettify()[:500])\n",
    "\n",
    "                # Look for the header containing \"Institutions\" or similar words\n",
    "                headers = laureate_soup.find_all(['span', 'h2'], string=lambda text: text and ('Institutions' in text or 'Academic career' in text or 'Affiliations' in text))\n",
    "\n",
    "                if headers:\n",
    "                    print(f\"Found {len(headers)} relevant headers for {name}\")\n",
    "                    for header in headers:\n",
    "                        # Look for the first <p> after the header containing institutions info\n",
    "                        institutions_section = header.find_next('p')  # Get the next <p> tag after the header\n",
    "                        break  # Only process the first \"Institutions\" section we find\n",
    "\n",
    "                if institutions_section:\n",
    "                    # Print the institutions-related paragraph\n",
    "                    print(f\"Institutions Information for {name}: {institutions_section.get_text()}\\n\")\n",
    "                else:\n",
    "                    print(f\"No institutions information found for {name}.\\n\")\n",
    "\n",
    "                # Sleep to avoid sending too many requests in a short time\n",
    "                time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db429e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
