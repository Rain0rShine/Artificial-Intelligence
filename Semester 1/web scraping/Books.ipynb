{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9b08478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "trendingAllTime = pd.DataFrame( columns=['Title', 'Author', 'Year Published', 'Number of Logs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bdca03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atomic Habits\n",
      "It Ends With Us\n",
      "The 48 Laws of Power\n",
      "The Subtle Art of Not Giving a F*ck\n",
      "Um casamento arranjado\n",
      "Rich Dad, Poor Dad\n",
      "Harry Potter and the Philosopher's Stone\n",
      "It Starts with Us\n",
      "Control Your Mind and Master Your Feelings\n",
      "Think and Grow Rich\n",
      "Latidos Que No Dije\n",
      "How to Win Friends and Influence People\n",
      "Twisted Love\n",
      "A Game of Thrones\n",
      "It\n",
      "A Court of Mist and Fury\n",
      "The Psychology of Money\n",
      "The Love Hypothesis\n",
      "Shatter Me\n",
      "Haunting Adeline\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Send a request to the website and get the HTML content\n",
    "url = 'https://openlibrary.org/trending/forever'  # Replace with the URL of the website you're scraping\n",
    "\n",
    "# Make a request to the website and check the status code\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    # Step 2: Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the h2 elements with the specific class\n",
    "    mydivs = soup.find_all(\"a\", class_=\"results\")\n",
    "\n",
    "    # Loop through each element and print the text\n",
    "    for blueText in mydivs:\n",
    "        print(blueText.get_text(strip=True)) # get_text() extracts the inner text, and strip() removes any extra whitespace\n",
    "\n",
    "        new_row = {'Title': blueText.get_text(strip=True)}\n",
    "        trendingAllTime = trendingAllTime._append(new_row, ignore_index=True)\n",
    "else:\n",
    "    print(f\"Error: Unable to retrieve the page, status code {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60206466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Title Author Year Published  \\\n",
      "0                                Atomic Habits    NaN            NaN   \n",
      "1                              It Ends With Us    NaN            NaN   \n",
      "2                         The 48 Laws of Power    NaN            NaN   \n",
      "3          The Subtle Art of Not Giving a F*ck    NaN            NaN   \n",
      "4                       Um casamento arranjado    NaN            NaN   \n",
      "5                           Rich Dad, Poor Dad    NaN            NaN   \n",
      "6     Harry Potter and the Philosopher's Stone    NaN            NaN   \n",
      "7                            It Starts with Us    NaN            NaN   \n",
      "8   Control Your Mind and Master Your Feelings    NaN            NaN   \n",
      "9                          Think and Grow Rich    NaN            NaN   \n",
      "10                         Latidos Que No Dije    NaN            NaN   \n",
      "11     How to Win Friends and Influence People    NaN            NaN   \n",
      "12                                Twisted Love    NaN            NaN   \n",
      "13                           A Game of Thrones    NaN            NaN   \n",
      "14                                          It    NaN            NaN   \n",
      "15                    A Court of Mist and Fury    NaN            NaN   \n",
      "16                     The Psychology of Money    NaN            NaN   \n",
      "17                         The Love Hypothesis    NaN            NaN   \n",
      "18                                  Shatter Me    NaN            NaN   \n",
      "19                            Haunting Adeline    NaN            NaN   \n",
      "\n",
      "   Number of Logs  \n",
      "0             NaN  \n",
      "1             NaN  \n",
      "2             NaN  \n",
      "3             NaN  \n",
      "4             NaN  \n",
      "5             NaN  \n",
      "6             NaN  \n",
      "7             NaN  \n",
      "8             NaN  \n",
      "9             NaN  \n",
      "10            NaN  \n",
      "11            NaN  \n",
      "12            NaN  \n",
      "13            NaN  \n",
      "14            NaN  \n",
      "15            NaN  \n",
      "16            NaN  \n",
      "17            NaN  \n",
      "18            NaN  \n",
      "19            NaN  \n"
     ]
    }
   ],
   "source": [
    "print(trendingAllTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bbfb56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016\n",
      "41\n",
      "2016—41 editions\n",
      "2012\n",
      "34\n",
      "2012—34 editions\n",
      "1998\n",
      "52\n",
      "1998—52 editions\n",
      "2016\n",
      "41\n",
      "2016—41 editions\n",
      "2019\n",
      "15\n",
      "2019—15 editions\n",
      "1990\n",
      "78\n",
      "1990—78 editions\n",
      "1997\n",
      "33\n",
      "1997—337 editions\n",
      "2022\n",
      "16\n",
      "2022—16 editions\n",
      "2019\n",
      "3 \n",
      "2019—3 editions\n",
      "1937\n",
      "27\n",
      "1937—279 editions\n",
      "2022\n",
      "2 \n",
      "2022—2 editions\n",
      "1913\n",
      "15\n",
      "1913—151 editions\n",
      "2021\n",
      "13\n",
      "2021—13 editions\n",
      "1996\n",
      "86\n",
      "1996—86 editions\n",
      "1925\n",
      "95\n",
      "1925—95 editions\n",
      "2014\n",
      "25\n",
      "2014—25 editions\n",
      "2020\n",
      "9 \n",
      "2020—9 editions\n",
      "2021\n",
      "7 \n",
      "2021—7 editions\n",
      "2011\n",
      "13\n",
      "2011—13 editions\n",
      "2021\n",
      "9 \n",
      "2021—9 editions\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Send a request to the website and get the HTML content\n",
    "url = 'https://openlibrary.org/trending/forever'  # Replace with the URL of the website you're scraping\n",
    "\n",
    "# Make a request to the website and check the status code\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    # Step 2: Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the h2 elements with the specific class\n",
    "    mydivs = soup.find_all(\"span\", class_=\"resultDetails\")\n",
    "\n",
    "    # Loop through each element and print the text\n",
    "    for blueText in mydivs:\n",
    "        step1 = blueText.get_text(strip=True).replace('First published in ', '', 1)\n",
    "        year = step1[0:4]\n",
    "        print(year)\n",
    "        editions = step1[5: 7]\n",
    "        print(editions)\n",
    "\n",
    "        print(blueText.get_text(strip=True).replace('First published in ', '', 1)) # get_text() extracts the inner text, and strip() removes any extra whitespace\n",
    "\n",
    "        new_row = {'Year Published': year}\n",
    "        trendingAllTime = trendingAllTime._append(new_row, ignore_index=True)\n",
    "        new_row = {'Year Published': year}\n",
    "        trendingAllTime = trendingAllTime._append(new_row, ignore_index=True)\n",
    "else:\n",
    "    print(f\"Error: Unable to retrieve the page, status code {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2ee72d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James Clear\n",
      "Colleen Hoover\n",
      "Robert GreeneandJoost Elffers\n",
      "Mark Manson\n",
      "Zana Kheiron\n",
      "Robert T. KiyosakiandSharon L. Lechter\n",
      "J. K. Rowling\n",
      "Colleen Hoover\n",
      "Eric Robertson - undifferentiated\n",
      "Napoleon Hill\n",
      "Roos\n",
      "Dale Carnegie\n",
      "Ana Huang\n",
      "George R. R. Martin\n",
      "Stephen King\n",
      "Sarah J. Maas\n",
      "Morgan Housel\n",
      "Ali Hazelwood\n",
      "Tahereh Mafi\n",
      "H. D. Carlton\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Send a request to the website and get the HTML content\n",
    "url = 'https://openlibrary.org/trending/forever'  # Replace with the URL of the website you're scraping\n",
    "\n",
    "# Make a request to the website and check the status code\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    # Step 2: Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the h2 elements with the specific class\n",
    "    mydivs = soup.find_all(\"span\", class_=\"bookauthor\")\n",
    "\n",
    "    # Loop through each element and print the text\n",
    "    for blueText in mydivs:\n",
    "        print(blueText.get_text(strip=True).replace('by', '', 1)) # get_text() extracts the inner text, and strip() removes any extra whitespace\n",
    "\n",
    "        new_row = {'Author': blueText.get_text(strip=True).replace('by', '', 1)}\n",
    "        trendingAllTime = trendingAllTime._append(new_row, ignore_index=True)\n",
    "else:\n",
    "    print(f\"Error: Unable to retrieve the page, status code {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eea147f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atomic HabitsbyJames ClearFirst published in 2016—41 editionsLogged 41181 times All Time\n",
      "61\n",
      " 41181 ti\n",
      "It Ends With UsbyColleen HooverFirst published in 2012—34 editionsLogged 40567 times All Time\n",
      "66\n",
      " 40567 ti\n",
      "The 48 Laws of PowerbyRobert GreeneandJoost ElffersFirst published in 1998—52 editionsLogged 34986 times All Time\n",
      "86\n",
      " 34986 ti\n",
      "The Subtle Art of Not Giving a F*ckbyMark MansonFirst published in 2016—41 editionsLogged 30182 times All Time\n",
      "83\n",
      " 30182 ti\n",
      "Um casamento arranjadobyZana KheironFirst published in 2019—15 editionsLogged 23272 times All Time\n",
      "71\n",
      " 23272 ti\n",
      "Rich Dad, Poor DadbyRobert T. KiyosakiandSharon L. LechterFirst published in 1990—78 editionsLogged 23002 times All Time\n",
      "93\n",
      " 23002 ti\n",
      "Harry Potter and the Philosopher's StonebyJ. K. RowlingFirst published in 1997—337 editionsLogged 15987 times All Time\n",
      "91\n",
      " 15987 ti\n",
      "It Starts with UsbyColleen HooverFirst published in 2022—16 editionsLogged 15547 times All Time\n",
      "68\n",
      " 15547 ti\n",
      "Control Your Mind and Master Your FeelingsbyEric Robertson - undifferentiatedFirst published in 2019—3 editionsLogged 15511 times All Time\n",
      "111\n",
      " 15511 ti\n",
      "Think and Grow RichbyNapoleon HillFirst published in 1937—279 editionsLogged 11716 times All Time\n",
      "70\n",
      " 11716 ti\n",
      "Latidos Que No DijebyRoosFirst published in 2022—2 editionsLogged 9778 times All Time\n",
      "59\n",
      " 9778 tim\n",
      "How to Win Friends and Influence PeoplebyDale CarnegieFirst published in 1913—151 editionsLogged 9226 times All Time\n",
      "90\n",
      " 9226 tim\n",
      "Twisted LovebyAna HuangFirst published in 2021—13 editionsLogged 8840 times All Time\n",
      "58\n",
      " 8840 tim\n",
      "A Game of ThronesbyGeorge R. R. MartinFirst published in 1996—86 editionsLogged 8596 times All Time\n",
      "73\n",
      " 8596 tim\n",
      "ItbyStephen KingFirst published in 1925—95 editionsLogged 7917 times All Time\n",
      "51\n",
      " 7917 tim\n",
      "A Court of Mist and FurybySarah J. MaasFirst published in 2014—25 editionsLogged 7827 times All Time\n",
      "74\n",
      " 7827 tim\n",
      "The Psychology of MoneybyMorgan HouselFirst published in 2020—9 editionsLogged 7760 times All Time\n",
      "72\n",
      " 7760 tim\n",
      "The Love HypothesisbyAli HazelwoodFirst published in 2021—7 editionsLogged 7097 times All Time\n",
      "68\n",
      " 7097 tim\n",
      "Shatter MebyTahereh MafiFirst published in 2011—13 editionsLogged 6849 times All Time\n",
      "59\n",
      " 6849 tim\n",
      "Haunting AdelinebyH. D. CarltonFirst published in 2021—9 editionsLogged 6823 times All Time\n",
      "65\n",
      " 6823 tim\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Send a request to the website and get the HTML content\n",
    "url = 'https://openlibrary.org/trending/forever'  # Replace with the URL of the website you're scraping\n",
    "\n",
    "# Make a request to the website and check the status code\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    # Step 2: Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the h2 elements with the specific class\n",
    "    mydivs = soup.find_all(\"div\", class_=\"details\")\n",
    "\n",
    "    # Loop through each element and print the text\n",
    "    for blueText in mydivs:\n",
    "        step0 = blueText.get_text(strip=True)\n",
    "        print(blueText.get_text(strip=True)) # get_text() extracts the inner text, and strip() removes any extra whitespace\n",
    "        print(step0.index('Logged'))\n",
    "        print(step0[step0.index('Logged') + 6: step0.index('Logged')+ 15])\n",
    "\n",
    "        \n",
    "else:\n",
    "    print(f\"Error: Unable to retrieve the page, status code {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5623869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Send a request to the website and get the HTML content\n",
    "url = 'https://openlibrary.org/trending/forever'  # Replace with the URL of the website you're scraping\n",
    "\n",
    "# Make a request to the website and check the status code\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    # Step 2: Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the h2 elements with the specific class\n",
    "    mydivs = soup.find_all(\"span\", class_=\"this\")\n",
    "\n",
    "    # Loop through each element and print the text\n",
    "    for blueText in mydivs:\n",
    "        print(blueText.get_text(strip=True)) # get_text() extracts the inner text, and strip() removes any extra whitespace\n",
    "        \n",
    "\n",
    "else:\n",
    "    print(f\"Error: Unable to retrieve the page, status code {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c97cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `page=2` not found.\n"
     ]
    }
   ],
   "source": [
    "?page=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4a23719",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'page' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26932\\894888143.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwhile\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://openlibrary.org/trending/forever?page='\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'page' is not defined"
     ]
    }
   ],
   "source": [
    "while page > 1:\n",
    "    url = 'https://openlibrary.org/trending/forever?page='+ str(page)\n",
    "    page += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251737d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "If I select a random trending book, how many editions is it likely to have? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
