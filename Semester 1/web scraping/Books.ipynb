{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6868ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "trendingAllTime = pd.DataFrame( columns=['Title', 'Author', 'Year Published', 'Number of Logs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "658d11ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atomic Habits\n",
      "It Ends With Us\n",
      "The 48 Laws of Power\n",
      "The Subtle Art of Not Giving a F*ck\n",
      "Um casamento arranjado\n",
      "Rich Dad, Poor Dad\n",
      "Harry Potter and the Philosopher's Stone\n",
      "It Starts with Us\n",
      "Control Your Mind and Master Your Feelings\n",
      "Think and Grow Rich\n",
      "Latidos Que No Dije\n",
      "How to Win Friends and Influence People\n",
      "Twisted Love\n",
      "A Game of Thrones\n",
      "It\n",
      "A Court of Mist and Fury\n",
      "The Psychology of Money\n",
      "The Love Hypothesis\n",
      "Shatter Me\n",
      "Haunting Adeline\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Send a request to the website and get the HTML content\n",
    "url = 'https://openlibrary.org/trending/forever'  # Replace with the URL of the website you're scraping\n",
    "\n",
    "# Make a request to the website and check the status code\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    # Step 2: Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the h2 elements with the specific class\n",
    "    mydivs = soup.find_all(\"a\", class_=\"results\")\n",
    "\n",
    "    # Loop through each element and print the text\n",
    "    for blueText in mydivs:\n",
    "        print(blueText.get_text(strip=True)) # get_text() extracts the inner text, and strip() removes any extra whitespace\n",
    "\n",
    "        new_row = {'Title': blueText.get_text(strip=True)}\n",
    "        trendingAllTime = trendingAllTime._append(new_row, ignore_index=True)\n",
    "else:\n",
    "    print(f\"Error: Unable to retrieve the page, status code {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2772931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Title Author Year Published  \\\n",
      "0                                Atomic Habits    NaN            NaN   \n",
      "1                              It Ends With Us    NaN            NaN   \n",
      "2                         The 48 Laws of Power    NaN            NaN   \n",
      "3          The Subtle Art of Not Giving a F*ck    NaN            NaN   \n",
      "4                       Um casamento arranjado    NaN            NaN   \n",
      "5                           Rich Dad, Poor Dad    NaN            NaN   \n",
      "6     Harry Potter and the Philosopher's Stone    NaN            NaN   \n",
      "7                            It Starts with Us    NaN            NaN   \n",
      "8   Control Your Mind and Master Your Feelings    NaN            NaN   \n",
      "9                          Think and Grow Rich    NaN            NaN   \n",
      "10                         Latidos Que No Dije    NaN            NaN   \n",
      "11     How to Win Friends and Influence People    NaN            NaN   \n",
      "12                                Twisted Love    NaN            NaN   \n",
      "13                           A Game of Thrones    NaN            NaN   \n",
      "14                                          It    NaN            NaN   \n",
      "15                    A Court of Mist and Fury    NaN            NaN   \n",
      "16                     The Psychology of Money    NaN            NaN   \n",
      "17                         The Love Hypothesis    NaN            NaN   \n",
      "18                                  Shatter Me    NaN            NaN   \n",
      "19                            Haunting Adeline    NaN            NaN   \n",
      "\n",
      "   Number of Logs  \n",
      "0             NaN  \n",
      "1             NaN  \n",
      "2             NaN  \n",
      "3             NaN  \n",
      "4             NaN  \n",
      "5             NaN  \n",
      "6             NaN  \n",
      "7             NaN  \n",
      "8             NaN  \n",
      "9             NaN  \n",
      "10            NaN  \n",
      "11            NaN  \n",
      "12            NaN  \n",
      "13            NaN  \n",
      "14            NaN  \n",
      "15            NaN  \n",
      "16            NaN  \n",
      "17            NaN  \n",
      "18            NaN  \n",
      "19            NaN  \n"
     ]
    }
   ],
   "source": [
    "print(trendingAllTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62cb6259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016—41 editions\n",
      "2012—34 editions\n",
      "1998—52 editions\n",
      "2016—41 editions\n",
      "2019—15 editions\n",
      "1990—78 editions\n",
      "1997—337 editions\n",
      "2022—16 editions\n",
      "2019—3 editions\n",
      "1937—279 editions\n",
      "2022—2 editions\n",
      "1913—151 editions\n",
      "2021—13 editions\n",
      "1996—86 editions\n",
      "1925—95 editions\n",
      "2014—25 editions\n",
      "2020—9 editions\n",
      "2021—7 editions\n",
      "2011—13 editions\n",
      "2021—9 editions\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Send a request to the website and get the HTML content\n",
    "url = 'https://openlibrary.org/trending/forever'  # Replace with the URL of the website you're scraping\n",
    "\n",
    "# Make a request to the website and check the status code\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    # Step 2: Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the h2 elements with the specific class\n",
    "    mydivs = soup.find_all(\"span\", class_=\"resultDetails\")\n",
    "\n",
    "    # Loop through each element and print the text\n",
    "    for blueText in mydivs:\n",
    "        print(blueText.get_text(strip=True).replace('First published in ', '', 1)) # get_text() extracts the inner text, and strip() removes any extra whitespace\n",
    "\n",
    "        new_row = {'Year Published': blueText.get_text(strip=True).replace('First published in ', '', 1)}\n",
    "        trendingAllTime = trendingAllTime._append(new_row, ignore_index=True)\n",
    "else:\n",
    "    print(f\"Error: Unable to retrieve the page, status code {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "126af06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James Clear\n",
      "Colleen Hoover\n",
      "Robert GreeneandJoost Elffers\n",
      "Mark Manson\n",
      "Zana Kheiron\n",
      "Robert T. KiyosakiandSharon L. Lechter\n",
      "J. K. Rowling\n",
      "Colleen Hoover\n",
      "Eric Robertson - undifferentiated\n",
      "Napoleon Hill\n",
      "Roos\n",
      "Dale Carnegie\n",
      "Ana Huang\n",
      "George R. R. Martin\n",
      "Stephen King\n",
      "Sarah J. Maas\n",
      "Morgan Housel\n",
      "Ali Hazelwood\n",
      "Tahereh Mafi\n",
      "H. D. Carlton\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Send a request to the website and get the HTML content\n",
    "url = 'https://openlibrary.org/trending/forever'  # Replace with the URL of the website you're scraping\n",
    "\n",
    "# Make a request to the website and check the status code\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    # Step 2: Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the h2 elements with the specific class\n",
    "    mydivs = soup.find_all(\"span\", class_=\"bookauthor\")\n",
    "\n",
    "    # Loop through each element and print the text\n",
    "    for blueText in mydivs:\n",
    "        print(blueText.get_text(strip=True).replace('by', '', 1)) # get_text() extracts the inner text, and strip() removes any extra whitespace\n",
    "\n",
    "        new_row = {'Author': blueText.get_text(strip=True).replace('by', '', 1)}\n",
    "        trendingAllTime = trendingAllTime._append(new_row, ignore_index=True)\n",
    "else:\n",
    "    print(f\"Error: Unable to retrieve the page, status code {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3bc872",
   "metadata": {},
   "outputs": [],
   "source": [
    ".replace('by', '', 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
