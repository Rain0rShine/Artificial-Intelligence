{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467aaeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82d59028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n",
      "Error: Unable to retrieve the page, status code 200\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['name', 'wins'])\n",
    "page = 1\n",
    "while page <= 24:\n",
    "    url = 'https://www.scrapethissite.com/pages/forms/?page_num='+ str(page)\n",
    "    \n",
    "    # Make a request to the website and check the status code\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Step 2: Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all the h2 elements with the specific class\n",
    "        mydivs = soup.find_all(\"td\", class_=\"name\")\n",
    "        mydivs2 = soup.find_all(\"td\", class_=\"wins\")\n",
    "        \n",
    "        # Loop through each element and print the text\n",
    "        for blueText in mydivs:\n",
    "            names = blueText.get_text(strip=True)\n",
    "            new_row = {'name' : names}\n",
    "            '''df = df._append(new_row, ignore_index=True)'''\n",
    "            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"Error: Unable to retrieve the page, status code {response.status_code}\")\n",
    "        for blueText in mydivs2:\n",
    "            wins = blueText.get_text(strip=True)\n",
    "            new_row = {'wins' : wins}\n",
    "            '''df = df._append(new_row, ignore_index=True)'''\n",
    "            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    \n",
    "    page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b30bcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    name wins\n",
      "0          Boston Bruins   44\n",
      "1         Buffalo Sabres   31\n",
      "2         Calgary Flames   46\n",
      "3     Chicago Blackhawks   49\n",
      "4      Detroit Red Wings   34\n",
      "..                   ...  ...\n",
      "577  Tampa Bay Lightning   38\n",
      "578  Toronto Maple Leafs   35\n",
      "579    Vancouver Canucks   51\n",
      "580  Washington Capitals   42\n",
      "581        Winnipeg Jets   37\n",
      "\n",
      "[582 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize an empty DataFrame with the columns\n",
    "df = pd.DataFrame(columns=['name', 'wins'])\n",
    "\n",
    "page = 1\n",
    "while page <= 24:\n",
    "    url = f'https://www.scrapethissite.com/pages/forms/?page_num={page}'\n",
    "    \n",
    "    # Make a request to the website and check the status code\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Step 2: Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all the td elements with the specific classes for names and wins\n",
    "        names = [name.get_text(strip=True) for name in soup.find_all(\"td\", class_=\"name\")]\n",
    "        wins = [win.get_text(strip=True) for win in soup.find_all(\"td\", class_=\"wins\")]\n",
    "        \n",
    "        # Check if the number of names and wins match\n",
    "        if len(names) == len(wins):\n",
    "            # Combine names and wins into a list of dictionaries\n",
    "            page_data = [{'name': name, 'wins': win} for name, win in zip(names, wins)]\n",
    "            \n",
    "            # Append the new data to the DataFrame\n",
    "            df = pd.concat([df, pd.DataFrame(page_data)], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"Data mismatch on page {page}. Names: {len(names)}, Wins: {len(wins)}\")\n",
    "    else:\n",
    "        print(f\"Error: Unable to retrieve the page, status code {response.status_code}\")\n",
    "    \n",
    "    page += 1\n",
    "\n",
    "# Output the final DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b87ca266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    name\n",
      "0          Boston Bruins\n",
      "1         Buffalo Sabres\n",
      "2         Calgary Flames\n",
      "3     Chicago Blackhawks\n",
      "4      Detroit Red Wings\n",
      "..                   ...\n",
      "577  Tampa Bay Lightning\n",
      "578  Toronto Maple Leafs\n",
      "579    Vancouver Canucks\n",
      "580  Washington Capitals\n",
      "581        Winnipeg Jets\n",
      "\n",
      "[582 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fd18146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446\n"
     ]
    }
   ],
   "source": [
    "df['wins'] = pd.to_numeric(df['wins'], errors='coerce')\n",
    "teams_over_30_wins = df[df['wins'] > 30]\n",
    "print(len(teams_over_30_wins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9796d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
